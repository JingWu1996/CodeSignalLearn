{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a Python list\n",
    "py_list = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Convert list to a Numpy array\n",
    "np_array = np.array(py_list)\n",
    "\n",
    "print(np_array) # Output: [1 2 3 4 5]\n",
    "\n",
    "# Create a 2D Python list\n",
    "py_list_2d = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "\n",
    "# Convert list to a Numpy array\n",
    "np_array_2d = np.array(py_list_2d)\n",
    "\n",
    "print(np_array_2d)\n",
    "# Output:\n",
    "# [[1 2 3]\n",
    "#  [4 5 6]\n",
    "#  [7 8 9]]\n",
    "\n",
    "np_array = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(\"Dimensions: \", np_array.ndim) # Dimensions:  2\n",
    "print(\"Shape: \", np_array.shape)     # Shape:  (2, 3)\n",
    "print(\"Size: \", np_array.size)       # Size: 6\n",
    "print(\"Data Type: \", np_array.dtype) # Data Type:  int64\n",
    "\n",
    "# Indexing: access the element at the first row, third column\n",
    "print(\"Indexed Value: \", np_array[0, 2]) # Indexed Value:  3\n",
    "\n",
    "# Slicing: access the first row \n",
    "print(\"Sliced Value: \", np_array[0,:]) # Sliced Value:  [1 2 3]\n",
    "\n",
    "# Reshape the array to 3 rows and 2 columns (only applicable if the reshaped total size equals the original size)\n",
    "reshaped_array = np_array.reshape(3, 2)\n",
    "print(\"Reshaped Array:\\n\", reshaped_array)\n",
    "# Reshaped Array:\n",
    "# [[1 2]\n",
    "#  [3 4]\n",
    "#  [5 6]]\n",
    "\n",
    "np_array1 = np.array([1, 2, 3])\n",
    "np_array2 = np.array([4, 5, 6])\n",
    "\n",
    "# Addition\n",
    "print(np_array1 + np_array2) # Output: [5 7 9]\n",
    "\n",
    "# Subtraction\n",
    "print(np_array1 - np_array2) # Output: [-3 -3 -3]\n",
    "\n",
    "# Multiplication\n",
    "print(np_array1 * np_array2) # Output: [4 10 18]\n",
    "\n",
    "# Division\n",
    "print(np_array1 / np_array2) # Output: [0.25 0.4 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "data_dict = {\"Name\": [\"John\", \"Anna\", \"Peter\"],\n",
    "             \"Age\": [28, 24, 33],\n",
    "             \"City\": [\"New York\", \"Los Angeles\", \"Berlin\"]}\n",
    "\n",
    "df = pd.DataFrame(data_dict)\n",
    "\n",
    "print(df)\n",
    "\n",
    "\"\"\"\n",
    "    Name  Age         City\n",
    "0   John   28     New York\n",
    "1   Anna   24  Los Angeles\n",
    "2  Peter   33       Berlin\n",
    "\"\"\"\n",
    "print(df.head(2))  # Print first two rows\n",
    "print(df.tail(2))  # Print last two rows\n",
    "print(df.shape)    # Print dimensions of the df (rows, columns): (3, 3)\n",
    "print(df.columns)  # Print column labels: Index(['Name', 'Age', 'City'], dtype='object')\n",
    "print(df.dtypes)   # Print data types of each column:\n",
    "# Name    object\n",
    "# Age      int64\n",
    "# City    object\n",
    "# dtype: object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda & apply function(IsChild)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"IsYouthful\"] = df[\"Age\"].apply(lambda age: \"Yes\" if age < 30 else \"No\")\n",
    "print(df)\n",
    "\n",
    "\"\"\"\n",
    "    Name  Age         City IsYouthful\n",
    "0   John   28     New York        Yes\n",
    "1   Anna   24  Los Angeles        Yes\n",
    "2  Peter   33       Berlin         No\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({\"Name\": [\"Megan\"], \"Age\": [34], \"City\": [\"San Francisco\"], \"IsYouthful\": [\"No\"]})\n",
    "\n",
    "df_concatenated = pd.concat([df, df2], ignore_index=True)\n",
    "\n",
    "print(df_concatenated)\n",
    "\n",
    "\"\"\"\n",
    "    Name  Age           City IsYouthful\n",
    "0   John   28       New York        Yes\n",
    "1   Anna   24    Los Angeles        Yes\n",
    "2  Peter   33         Berlin         No\n",
    "3  Megan   34  San Francisco         No (reset index cause ignore_index=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['column_name']) # select a single column\n",
    "print(df[['col1', 'col2']]) # select multiple columns\n",
    "\n",
    "#df.iloc[row_selection, column_selection]:\n",
    "df.iloc[1,0] # Select the value in the second row and the first column (1-based)\n",
    "df.iloc[:2,:2] # Select the first two rows and columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic Pandas|Titanic data from Seaborn lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the titanic dataset into a Pandas DataFrame\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# Look at the first 3 rows of the DataFrame\n",
    "print(titanic.head(3))\n",
    "\n",
    "\"\"\"\n",
    "   survived  pclass     sex   age  ...  deck  embark_town  alive  alone\n",
    "0         0       3    male  22.0  ...   NaN  Southampton     no  False\n",
    "1         1       1  female  38.0  ...     C    Cherbourg    yes  False\n",
    "2         1       3  female  26.0  ...   NaN  Southampton    yes   True\n",
    "\n",
    "[3 rows x 15 columns]\n",
    "\"\"\"\n",
    "\n",
    "# Using lambda for DataFrame manipulation\n",
    "# Create a new column, \"IsChild\", to mark the passengers who are under 18\n",
    "titanic[\"IsChild\"] = titanic[\"age\"].apply(lambda age: \"Yes\" if age < 18 else \"No\")\n",
    "print(\"\\nDataFrame after adding the 'IsChild' column:\")\n",
    "print(titanic.head(5))\n",
    "\n",
    "# less than 7，under 7, 表示＜7\n",
    "# not more than 7，表示≤7\n",
    "\n",
    "# Concatenating DataFrames\n",
    "# Create a new DataFrame\n",
    "new_data = pd.DataFrame({\"survived\": [1],\n",
    "                         \"pclass\": [3],\n",
    "                         \"sex\": [\"male\"],\n",
    "                         \"age\": [32],\n",
    "                         \"sibsp\": [0],\n",
    "                         \"parch\": [0],\n",
    "                         \"fare\": [7.75],\n",
    "                         \"embarked\": [\"Q\"],\n",
    "                         \"class\": [\"Third\"],\n",
    "                         \"who\": [\"man\"],\n",
    "                         \"adult_male\": [True],\n",
    "                         \"deck\": [None],\n",
    "                         \"embark_town\": [\"Queenstown\"],\n",
    "                         \"alive\": [\"yes\"],\n",
    "                         \"alone\": [True],\n",
    "                         \"IsChild\": [\"No\"]})\n",
    "# Drop columns with all-NA values\n",
    "# To Handle the Future warning of concat function (since it will not support NA)\n",
    "new_data = new_data.dropna(axis=1, how='all')\n",
    "# Concatenate the new data to the original DataFrame\n",
    "titanic_concat = pd.concat([titanic, new_data], ignore_index=True)\n",
    "print(\"\\nConcatenated DataFrame:\")\n",
    "print(titanic_concat.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# descriptive statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Load Titanic dataset\n",
    "titanic_df = sns.load_dataset('titanic')\n",
    "\n",
    "mean_age = titanic_df['age'].mean()\n",
    "median_age = titanic_df['age'].median()\n",
    "mode_age = titanic_df['age'].mode()[0] #因为返回的是一个Series所以需要index[0]\n",
    "\n",
    "print(f\"Mean age: {mean_age}\") # Mean age: 29.69911764705882\n",
    "print(f\"Median age: {median_age}\") # Median age: 28.0\n",
    "print(f\"Mode age: {mode_age}\") # Mode age: 24.0\n",
    "# Standard deviation\n",
    "std_dev_age = np.std(titanic_df['age'])\n",
    "\n",
    "print(f\"Standard deviation of age: {std_dev_age}\") # Standard deviation of age: 14.516321150817316"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个 Series\n",
    "data = pd.Series([1, 2, 2, 3, 4, 4, 4, 5])\n",
    "\n",
    "# 众数（mode），Series的众数返回Series\n",
    "# 指定了列的时候返回某个数，没指定时返回各个列的众数为一个dataframe\n",
    "# 多个值出现次数相同时，返回多个值\n",
    "mode_value = data.mode()\n",
    "print(mode_value)  # 输出: 0    4\n",
    "                   #       dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四分位，百分位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quartiles and percentiles\n",
    "# Using Numpy\n",
    "Q1_age_np = np.percentile(titanic_df['age'].dropna(), 25) # dropna is being used to drop NA values\n",
    "Q3_age_np = np.percentile(titanic_df['age'].dropna(), 75) # Numpy的Percentile需要DropNA一下\n",
    "\n",
    "print(f\"First quartile of age (Numpy): {Q1_age_np}\")\n",
    "print(f\"Third quartile of age (Numpy): {Q3_age_np}\")\n",
    "\n",
    "# Output:\n",
    "# First quartile of age (Numpy): 20.125 ->有25%的人的年龄低于20.125岁\n",
    "# Third quartile of age (Numpy): 38.0   ->有75%的人的年龄低于38岁\n",
    "\n",
    "# Using Pandas\n",
    "Q1_age_pd = titanic_df['age'].quantile(0.25)\n",
    "Q3_age_pd = titanic_df['age'].quantile(0.75) # pandas的Quantile不需要DropNA\n",
    "\n",
    "print(f\"First quartile of age (Pandas): {Q1_age_pd}\")\n",
    "print(f\"Third quartile of age (Pandas): {Q3_age_pd}\")\n",
    "\n",
    "# Output:\n",
    "# First quartile of age (Pandas): 20.125\n",
    "# Third quartile of age (Pandas): 38.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering and Sorting(Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "titanic_df = sns.load_dataset('titanic')\n",
    "\n",
    "# Filter passengers who survived\n",
    "survivors = titanic_df[titanic_df['survived'] == 1]\n",
    "print(survivors.head())\n",
    "\n",
    "\"\"\"\n",
    "   survived  pclass     sex   age  ...  deck  embark_town  alive  alone\n",
    "1         1       1  female  38.0  ...     C    Cherbourg    yes  False\n",
    "2         1       3  female  26.0  ...   NaN  Southampton    yes   True\n",
    "3         1       1  female  35.0  ...     C  Southampton    yes  False\n",
    "8         1       3  female  27.0  ...   NaN  Southampton    yes  False\n",
    "9         1       2  female  14.0  ...   NaN    Cherbourg    yes  False\n",
    "\n",
    "[5 rows x 15 columns]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort survivors by age\n",
    "sorted_df = survivors.sort_values('age') # ascending 升序\n",
    "print(sorted_df.head())\n",
    "\n",
    "\"\"\"\n",
    "     survived  pclass     sex   age  ...  deck  embark_town  alive  alone\n",
    "803         1       3    male  0.42  ...   NaN    Cherbourg    yes  False\n",
    "755         1       2    male  0.67  ...   NaN  Southampton    yes  False\n",
    "644         1       3  female  0.75  ...   NaN    Cherbourg    yes  False\n",
    "469         1       3  female  0.75  ...   NaN    Cherbourg    yes  False\n",
    "831         1       2    male  0.83  ...   NaN  Southampton    yes  False\n",
    "\n",
    "[5 rows x 15 columns]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sort survivors by class and age\n",
    "sorted_df = survivors.sort_values(['pclass', 'age'], ascending=[False, True])\n",
    "print(sorted_df.head())\n",
    "\n",
    "\"\"\"\n",
    "     survived  pclass     sex   age  ...  deck  embark_town  alive  alone\n",
    "803         1       3    male  0.42  ...   NaN    Cherbourg    yes  False\n",
    "469         1       3  female  0.75  ...   NaN    Cherbourg    yes  False\n",
    "644         1       3  female  0.75  ...   NaN    Cherbourg    yes  False\n",
    "172         1       3  female  1.00  ...   NaN  Southampton    yes  False\n",
    "381         1       3  female  1.00  ...   NaN    Cherbourg    yes  False\n",
    "\n",
    "[5 rows x 15 columns]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter female passengers who survived\n",
    "female_survivors = titanic_df[\n",
    "    (titanic_df['survived'] == 1) & (titanic_df['sex'] == 'female')\n",
    "]\n",
    "print(female_survivors.head())\n",
    "\n",
    "\"\"\"\n",
    "   survived  pclass     sex   age  ...  deck  embark_town  alive  alone\n",
    "1         1       1  female  38.0  ...     C    Cherbourg    yes  False\n",
    "2         1       3  female  26.0  ...   NaN  Southampton    yes   True\n",
    "3         1       1  female  35.0  ...     C  Southampton    yes  False\n",
    "8         1       3  female  27.0  ...   NaN  Southampton    yes  False\n",
    "9         1       2  female  14.0  ...   NaN    Cherbourg    yes  False\n",
    "\n",
    "[5 rows x 15 columns]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "titanic_df = sns.load_dataset('titanic')\n",
    "\n",
    "# Detect missing values \n",
    "missing_values = titanic_df.isnull() # return boolean dataframe, same size as original data\n",
    "print(missing_values.head(10))\n",
    "\"\"\"\n",
    "   survived  pclass    sex    age  ...   deck  embark_town  alive  alone\n",
    "0     False   False  False  False  ...   True        False  False  False\n",
    "1     False   False  False  False  ...  False        False  False  False\n",
    "2     False   False  False  False  ...   True        False  False  False\n",
    "3     False   False  False  False  ...  False        False  False  False\n",
    "4     False   False  False  False  ...   True        False  False  False\n",
    "5     False   False  False   True  ...   True        False  False  False\n",
    "6     False   False  False  False  ...  False        False  False  False\n",
    "7     False   False  False  False  ...   True        False  False  False\n",
    "8     False   False  False  False  ...   True        False  False  False\n",
    "9     False   False  False  False  ...   True        False  False  False\n",
    "\n",
    "[10 rows x 15 columns]\n",
    "\"\"\"\n",
    "missing_values_count = titanic_df.isnull().sum()\n",
    "print(missing_values_count)\n",
    "\"\"\"\n",
    "survived         0\n",
    "pclass           0\n",
    "sex              0\n",
    "age            177\n",
    "sibsp            0\n",
    "parch            0\n",
    "fare             0\n",
    "embarked         2\n",
    "class            0\n",
    "who              0\n",
    "adult_male       0\n",
    "deck           688\n",
    "embark_town      2\n",
    "alive            0\n",
    "alone            0\n",
    "dtype: int64\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Detected missing values visualized\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(titanic_df.isnull(), cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop的前提是缺失数据少，不影响我们的计算\n",
    "# Copy the original dataset\n",
    "titanic_df_copy = titanic_df.copy()\n",
    "\n",
    "# Drop rows with missing values\n",
    "titanic_df_copy.dropna(inplace=True) # titanic_df_copy is now modified\n",
    "titanic_df_copy = titanic_df.dropna(inplace=False) # Original df remains the same; you get new_df with modifications\n",
    "\n",
    "# Check the dataframe\n",
    "print(titanic_df_copy.isnull().sum())\n",
    "# There will be no missing values in every column\n",
    "\n",
    "print(titanic_df.shape)\n",
    "# TODO: Clean the dataset to remove rows with missing 'age' data\n",
    "titanic_df['age_missing'] = titanic_df['age'].isnull()\n",
    "print(titanic_df['age_missing'].unique()) \n",
    "\n",
    "missing_values_count = titanic_df['age'].isnull().sum()\n",
    "print(missing_values_count)\n",
    "\n",
    "titanic_df_removed = titanic_df[titanic_df['age_missing'] == False]\n",
    "print(titanic_df_removed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation估算适用于不想改变数据整体大小，因此用平均值，中位数，众数等等来取代缺损\n",
    "# Impute missing values using mean\n",
    "titanic_df['age'].fillna(titanic_df['age'].mean(), inplace=True)\n",
    "\n",
    "# Check the dataframe\n",
    "print(titanic_df.isnull().sum())\n",
    "\"\"\"\n",
    "survived         0\n",
    "pclass           0\n",
    "sex              0\n",
    "age              0\n",
    "sibsp            0\n",
    "parch            0\n",
    "fare             0\n",
    "embarked         2\n",
    "class            0\n",
    "who              0\n",
    "adult_male       0\n",
    "deck           688\n",
    "embark_town      2\n",
    "alive            0\n",
    "alone            0\n",
    "dtype: int64\n",
    "\"\"\"\n",
    "# Impute missing values using backward fill\n",
    "titanic_df['age'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Check the dataframe\n",
    "print(titanic_df.isnull().sum())\n",
    "# The output is the same as in the previous example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Data Encoding 类型特征编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Load Titanic dataset\n",
    "titanic_df = sns.load_dataset('titanic')\n",
    "# Display unique categories in 'sex' and 'embark_town'\n",
    "print(titanic_df['sex'].unique()) # Output: ['male' 'female']\n",
    "print(titanic_df['embark_town'].unique()) # Output: ['Southampton' 'Cherbourg' 'Queenstown' nan]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### factorize(Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding for 'sex'\n",
    "titanic_df['sex_encoded'] = pd.factorize(titanic_df['sex'])[0]\n",
    "# factorize返回2个item，index0是numerical encoded label，index1是unique value array\n",
    "# 下面显示了index0\n",
    "print(titanic_df[['sex', 'sex_encoded']].head())\n",
    "\"\"\"\n",
    "      sex  sex_encoded\n",
    "0    male            0\n",
    "1  female            1\n",
    "2  female            1\n",
    "3  female            1\n",
    "4    male            0\n",
    "\"\"\"\n",
    "# 因为male最先出现，所以被factorize函数label为0，female被label为1\n",
    "\n",
    "# 如果想要指定label可以直接用replace函数\n",
    "# Replace 'yes' with 1 and 'no' with 0\n",
    "titanic_df['alive_encoded'] = titanic_df['alive'].replace({'yes': 1, 'no': 0})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_dummies(One-Hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding for 'embark_town'\n",
    "encoded_df = pd.get_dummies(titanic_df['embark_town'], prefix='town')\n",
    "# get_dummies为变量embark_town的每个类别，创建了一个binary column(TF=10)，适用于没有排序性的类别\n",
    "\n",
    "titanic_df = pd.concat([titanic_df, encoded_df], axis=1)\n",
    "print(titanic_df.head())\n",
    "\"\"\"\n",
    "   survived  pclass     sex  ...  town_Cherbourg  town_Queenstown  town_Southampton\n",
    "0         0       3    male  ...           False            False              True\n",
    "1         1       1  female  ...            True            False             False\n",
    "2         1       3  female  ...           False            False              True\n",
    "3         1       1  female  ...           False            False              True\n",
    "4         0       3    male  ...           False            False              True\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling(numerical -> diff ranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaler(Normal distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset and drop rows with missing values\n",
    "titanic_df = sns.load_dataset('titanic').dropna()\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the 'age' column\n",
    "titanic_df['age_std'] = std_scaler.fit_transform(np.array(titanic_df['age']).reshape(-1, 1))\n",
    "# reshape使得age列从1D变成了2D array\n",
    "# 下面的代码实现了同样的功能，但是直接将age列变成了一个dataframe\n",
    "titanic_df['age_std'] = std_scaler.fit_transform(titanic_df[['age']])\n",
    "\n",
    "# Check the transformed 'age' column\n",
    "print(titanic_df['age','age_std'].head())\n",
    "\"\"\"\n",
    "1     0.152082\n",
    "3    -0.039875\n",
    "6     1.175852\n",
    "10   -2.023430\n",
    "11    1.431795\n",
    "Name: age, dtype: float64\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data is normally distributed?\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import shapiro, kstest, anderson\n",
    "\n",
    "# Shapiro-Wilk Test(small sample)\n",
    "stat, p = shapiro(titanic_df['age'])\n",
    "print(f'Shapiro-Wilk test p-value: {p}')\n",
    "\n",
    "# Kolmogorov-Smirnov Test(larger sample)\n",
    "stat, p = kstest(titanic_df['age'], 'norm', args=(titanic_df['age'].mean(), titanic_df['age'].std()))\n",
    "print(f'Kolmogorov-Smirnov test p-value: {p}')\n",
    "\n",
    "# Anderson-Darling Test(powerful)\n",
    "result = anderson(titanic_df['age'])\n",
    "print(f'Anderson-Darling test statistic: {result.statistic}')\n",
    "\n",
    "# Histogram\n",
    "sns.histplot(data=titanic_df['age'], kde=True)\n",
    "plt.show()\n",
    "\n",
    "# QQ Plot\n",
    "stats.probplot(titanic_df['age'], dist=\"norm\", plot=plt)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-Max Scaler(No specific Shape/Distribution)归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the 'fare' column\n",
    "titanic_df['fare'] = min_max_scaler.fit_transform(np.array(titanic_df['fare']).reshape(-1, 1))\n",
    "\n",
    "# Check the transformed 'fare' column\n",
    "print(titanic_df['fare'].head())\n",
    "\"\"\"\n",
    "1     0.139136\n",
    "3     0.103644\n",
    "6     0.101229\n",
    "10    0.032596\n",
    "11    0.051822\n",
    "Name: fare, dtype: float64\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust Scaler(四分位IQR 抵御异常值影响)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Initialize the RobustScaler\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "# Fit and transform the 'fare' column\n",
    "titanic_df['fare'] = robust_scaler.fit_transform(np.array(titanic_df['fare']).reshape(-1, 1))\n",
    "\n",
    "# Check the transformed 'fare' column\n",
    "print(titanic_df['fare'].head())\n",
    "\"\"\"\n",
    "1     0.236871\n",
    "3    -0.064677\n",
    "6    -0.085199\n",
    "10   -0.668325\n",
    "11   -0.504975\n",
    "Name: fare, dtype: float64\n",
    "\"\"\"\n",
    "# 异常值变成了small positive and nagative values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "titanic_df = sns.load_dataset('titanic')\n",
    "\n",
    "# Calculate Z-scores\n",
    "titanic_df['age_zscore'] = np.abs((titanic_df.age - titanic_df.age.mean()) / titanic_df.age.std())\n",
    "\n",
    "# Get rows of outliers according to the Z-score method (using a threshold of 3)\n",
    "outliers_zscore = titanic_df[(titanic_df['age_zscore'] > 3)]\n",
    "print(\"Outliers detected by the Z-score method:\")\n",
    "print(outliers_zscore)\n",
    "\"\"\"\n",
    "     survived  pclass   sex   age  ...  embark_town  alive  alone age_zscore\n",
    "630         1       1  male  80.0  ...  Southampton    yes   True   3.462699\n",
    "851         0       3  male  74.0  ...  Southampton     no   True   3.049660\n",
    "\n",
    "[2 rows x 16 columns]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IQR(25%~75%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IQR\n",
    "Q1 = titanic_df['age'].quantile(0.25)\n",
    "Q3 = titanic_df['age'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define Bounds\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Get rows of outliers according to IQR method\n",
    "outliers_iqr = titanic_df[(titanic_df['age'] < lower_bound) | (titanic_df['age'] > upper_bound)]\n",
    "print(outliers_iqr)\n",
    "\"\"\"\n",
    "     survived  pclass   sex   age  ...  embark_town  alive  alone age_zscore\n",
    "33          0       2  male  66.0  ...  Southampton     no   True   2.498943\n",
    "54          0       1  male  65.0  ...    Cherbourg     no  False   2.430103\n",
    "96          0       1  male  71.0  ...    Cherbourg     no   True   2.843141\n",
    "116         0       3  male  70.5  ...   Queenstown     no   True   2.808721\n",
    "280         0       3  male  65.0  ...   Queenstown     no   True   2.430103\n",
    "456         0       1  male  65.0  ...  Southampton     no   True   2.430103\n",
    "493         0       1  male  71.0  ...    Cherbourg     no   True   2.843141\n",
    "630         1       1  male  80.0  ...  Southampton    yes   True   3.462699\n",
    "672         0       2  male  70.0  ...  Southampton     no   True   2.774301\n",
    "745         0       1  male  70.0  ...  Southampton     no  False   2.774301\n",
    "851         0       3  male  74.0  ...  Southampton     no   True   3.049660\n",
    "\n",
    "[11 rows x 16 columns]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Or Replace?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Z-score method\n",
    "titanic_df = titanic_df[titanic_df['age_zscore'] <= 3]\n",
    "\n",
    "# Using the IQR method\n",
    "titanic_df = titanic_df[(titanic_df['age'] >= lower_bound) & (titanic_df['age'] <= upper_bound)]\n",
    "\n",
    "# using mean\n",
    "titanic_df.loc[titanic_df['age_zscore'] > 3, 'age'] = titanic_df['age'].mean()\n",
    "\n",
    "# using median\n",
    "# Handle outliers in the 'age' column detected by the Z-score method by replacing them with median\n",
    "titanic_clean = titanic_df\n",
    "titanic_clean.loc[titanic_clean['age_zscore'] > 3, 'age'] = titanic_df['age'].median()\n",
    "\n",
    "# Print cleaned data\n",
    "print(\"\\nData after handling outliers detected by Z-score method:\")\n",
    "print(titanic_clean)\n",
    "\n",
    "titanic_df.loc[(titanic_df['age'] < lower_bound) | (titanic_df['age'] > upper_bound), 'age'] = titanic_df['age'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Coeffcient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "titanic_df = sns.load_dataset('titanic')\n",
    "\n",
    "# Calculate and print the correlation matrix\n",
    "corr_matrix = titanic_df.corr(numeric_only=True)\n",
    "# corr function returns a DataFrame with the correlation coefficients between all pairs of numeric columns in titanic_df.\n",
    "\n",
    "print(corr_matrix)\n",
    "\"\"\"\n",
    "            survived    pclass       age  ...      fare  adult_male     alone\n",
    "survived    1.000000 -0.338481 -0.077221  ...  0.257307   -0.557080 -0.203367\n",
    "pclass     -0.338481  1.000000 -0.369226  ... -0.549500    0.094035  0.135207\n",
    "age        -0.077221 -0.369226  1.000000  ...  0.096067    0.280328  0.198270\n",
    "sibsp      -0.035322  0.083081 -0.308247  ...  0.159651   -0.253586 -0.584471\n",
    "parch       0.081629  0.018443 -0.189119  ...  0.216225   -0.349943 -0.583398\n",
    "fare        0.257307 -0.549500  0.096067  ...  1.000000   -0.182024 -0.271832\n",
    "adult_male -0.557080  0.094035  0.280328  ... -0.182024    1.000000  0.404744\n",
    "alone      -0.203367  0.135207  0.198270  ... -0.271832    0.404744  1.000000\n",
    "\n",
    "[8 rows x 8 columns]\n",
    "\"\"\"\n",
    "# -0.549500 as the passenger class decreases (3rd class to 1st class), the ticket fare increases.\n",
    "print(corr_matrix.loc['age','pclass'])\n",
    "\n",
    "# If 'fare' and 'pclass' are highly correlated\n",
    "clean_df = titanic_df.drop('fare', axis=1)\n",
    "# dropping the fare column. The axis=1 parameter indicates that we want to drop a column (for dropping a row, we would have used axis=0). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "titanic_df = sns.load_dataset('titanic')\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = titanic_df.corr(numeric_only=True)\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "\n",
    "# Add a title\n",
    "plt.title('Heatmap of the Correlation Matrix')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    " larger families might have a lower chance of survival due to difficulties keeping the family together during the sinking, or certain age groups might have a higher or lower survival rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# family_size, by adding sibsp (number of siblings/spouses aboard) and parch (number of parents/children aboard)\n",
    "# Load the data\n",
    "import seaborn as sns\n",
    "\n",
    "titanic_df = sns.load_dataset('titanic')\n",
    "\n",
    "# Create a new feature, 'family_size'\n",
    "titanic_df['family_size'] = titanic_df['sibsp'] + titanic_df['parch'] + 1 # plus one (the passenger themself)\n",
    "print(titanic_df.head())\n",
    "\"\"\"\n",
    "   survived  pclass     sex   age  ...  embark_town  alive  alone family_size\n",
    "0         0       3    male  22.0  ...  Southampton     no  False           2\n",
    "1         1       1  female  38.0  ...    Cherbourg    yes  False           2\n",
    "2         1       3  female  26.0  ...  Southampton    yes   True           1\n",
    "3         1       1  female  35.0  ...  Southampton    yes  False           2\n",
    "4         0       3    male  35.0  ...  Southampton     no   True           1\n",
    "\n",
    "[5 rows x 16 columns]\n",
    "\"\"\"\n",
    "\n",
    "# Create a new feature, 'is_alone'\n",
    "titanic['is_alone'] = 1\n",
    "titanic.loc[titanic['family_size'] > 1, 'is_alone'] = 0\n",
    "# Print the first 5 rows of the dataset\n",
    "print(titanic[['sibsp', 'parch', 'family_size', 'is_alone']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Continuous -> categorical(Pandas cut())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Define the bin edges\n",
    "age_bins = [0, 12, 18, 30, 45, 100]\n",
    "\n",
    "# Define the bin labels\n",
    "age_labels = ['Child', 'Teenager', 'Young Adult', 'Middle Age', 'Senior']\n",
    "\n",
    "# 标签一定比bin的edge少一个！！！Bin labels must be one fewer than the number of bin edges\n",
    "\n",
    "# Create the age group feature\n",
    "titanic_df['age_group'] = pd.cut(titanic_df['age'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "# Show the first few rows of the data\n",
    "print(titanic_df.head())\n",
    "\"\"\"\n",
    "   survived  pclass     sex   age  ...  alive  alone  family_size    age_group\n",
    "0         0       3    male  22.0  ...     no  False            2  Young Adult\n",
    "1         1       1  female  38.0  ...    yes  False            2   Middle Age\n",
    "2         1       3  female  26.0  ...    yes   True            1  Young Adult\n",
    "3         1       1  female  35.0  ...    yes  False            2   Middle Age\n",
    "4         0       3    male  35.0  ...     no   True            1   Middle Age\n",
    "\n",
    "[5 rows x 17 columns]\n",
    "\"\"\"\n",
    "# Check the distribution of the 'age_group' column\n",
    "print(titanic_df['age_group'].value_counts())\n",
    "\"\"\"\n",
    "age_group\n",
    "Young Adult    270\n",
    "Middle Age     202\n",
    "Senior         103\n",
    "Teenager        70\n",
    "Child           69\n",
    "Name: count, dtype: int64\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy,Pandas Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "\n",
    "dataset = fetch_california_housing()\n",
    "\n",
    "df = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)\n",
    "df[\"MedHouseValue\"] = dataset.target\n",
    "# 以上两行可以用以下一行完成\n",
    "df = pd.DataFrame(data=np.c_[data['data'], data['target']], # c_ 函数列结合，r_行结合\n",
    "                  columns=data['feature_names'] + ['MedHouseValue'])\n",
    "print(np.c_[np.array([1,2,3]),np.array([4,5,6])])\n",
    "\"\"\"\n",
    "[[1 4]\n",
    " [2 5]\n",
    " [3 6]]\n",
    " \"\"\"\n",
    "\n",
    "print(np.r_[np.array([1,2,3]),np.array([4,5,6])])\n",
    "\"\"\"\n",
    "[1 2 3 4 5 6]\n",
    "\"\"\"\n",
    "\n",
    "print(df.head())\n",
    "\"\"\"\n",
    "   MedInc  HouseAge  AveRooms  ...  Latitude  Longitude  MedHouseValue\n",
    "0  8.3252      41.0  6.984127  ...     37.88    -122.23          4.526\n",
    "1  8.3014      21.0  6.238137  ...     37.86    -122.22          3.585\n",
    "2  7.2574      52.0  8.288136  ...     37.85    -122.24          3.521\n",
    "3  5.6431      52.0  5.817352  ...     37.85    -122.25          3.413\n",
    "4  3.8462      52.0  6.281853  ...     37.85    -122.25          3.422\n",
    "\n",
    "[5 rows x 9 columns]\n",
    "\"\"\"\n",
    "\n",
    "print(\"Size of the dataframe: \", df.shape) # Output: (20640, 9)\n",
    "print(\"\\nStatistical Summary for the dataset:\")\n",
    "print(df.describe())\n",
    "\"\"\"\n",
    "             MedInc      HouseAge  ...     Longitude  MedHouseValue\n",
    "count  20640.000000  20640.000000  ...  20640.000000   20640.000000\n",
    "mean       3.870671     28.639486  ...   -119.569704       2.068558\n",
    "std        1.899822     12.585558  ...      2.003532       1.153956\n",
    "min        0.499900      1.000000  ...   -124.350000       0.149990\n",
    "25%        2.563400     18.000000  ...   -121.800000       1.196000\n",
    "50%        3.534800     29.000000  ...   -118.490000       1.797000\n",
    "75%        4.743250     37.000000  ...   -118.010000       2.647250\n",
    "max       15.000100     52.000000  ...   -114.310000       5.000010\n",
    "\n",
    "[8 rows x 9 columns]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MedInc: This is the median income for households within a block (scaled and capped at 15 for higher median \n",
    "incomes and at 0.5 for lower median incomes).\n",
    "\n",
    "HouseAge: This is the median house age within a block.\n",
    "\n",
    "AveRooms: This is the average number of rooms in the houses within a block.\n",
    "\n",
    "AveBedrms: This is the average number of bedrooms in the houses within a block.\n",
    "\n",
    "Population: This is the total population within a block.\n",
    "\n",
    "AveOccup: This is the average house occupancy, computed as the total population within a block divided by the number of households.\n",
    "\n",
    "Latitude and Longitude: These are the geographic coordinates of the block groups.\n",
    "\n",
    "MedHouseValue: This is the median house value for households within a block (measured in 100,000s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nChecking for missing values in the dataset:\")\n",
    "print(df.isnull().sum())\n",
    "\"\"\"\n",
    "MedInc           0\n",
    "HouseAge         0\n",
    "AveRooms         0\n",
    "AveBedrms        0\n",
    "Population       0\n",
    "AveOccup         0\n",
    "Latitude         0\n",
    "Longitude        0\n",
    "MedHouseValue    0\n",
    "dtype: int64\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### +，-，*，/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "matrix_a = np.random.randint(255, size=(3, 3))\n",
    "\n",
    "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(A)\n",
    "\"\"\"\n",
    "[[1 2 3]\n",
    " [4 5 6]\n",
    " [7 8 9]]\n",
    "\"\"\"\n",
    "B = np.array([[9, 8, 7], [6, 5, 4], [3, 2, 1]])\n",
    "\"\"\"\n",
    "[[9 8 7]\n",
    " [6 5 4]\n",
    " [3 2 1]]\n",
    "\"\"\"\n",
    "\n",
    "C = A + B\n",
    "print(C)\n",
    "\"\"\"\n",
    "[[10 10 10]\n",
    " [10 10 10]\n",
    " [10 10 10]]\n",
    "\"\"\"\n",
    "D = A - B\n",
    "print(D)\n",
    "\"\"\"\n",
    "[[-8 -6 -4]\n",
    " [-2  0  2]\n",
    " [ 4  6  8]]\n",
    "\"\"\"\n",
    "\n",
    "E = A * B\n",
    "print(E)\n",
    "# 要注意！E_ij = A_ij * B_ij ->element-wise multiplication!!!\n",
    "# 不同于线性代数中的矩阵乘法\n",
    "\"\"\"\n",
    "[[ 9 16 21]\n",
    " [24 25 24]\n",
    " [21 16  9]]\n",
    "\"\"\"\n",
    "# Matrix multiplication with @\n",
    "F = A @ B\n",
    "\n",
    "# Matrix multiplication with np.dot()\n",
    "G = np.dot(A, B)\n",
    "\n",
    "F = np.linalg.inv(E)  # Finding the inverse of matrix E\n",
    "# Only invertible可逆矩阵\n",
    "print(F)\n",
    "\"\"\"\n",
    "[[-0.73611111  0.88888889 -0.65277778]\n",
    " [ 1.33333333 -1.66666667  1.33333333]\n",
    " [-0.65277778  0.88888889 -0.73611111]]\n",
    "\"\"\"\n",
    "\n",
    "print(np.dot(E,F))\n",
    "\"\"\"\n",
    "[[1. 0. 0.]\n",
    " [0. 1. 0.]\n",
    " [0. 0. 1.]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular and Degenerate 奇异矩阵和退化矩阵只能求伪逆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.linalg.inv(A) doesn't exist, it will throw an exception\n",
    "FP = np.linalg.pinv(A)  # Finding the pseudo-inverse matrix of A\n",
    "print(FP)\n",
    "\"\"\"\n",
    "[[-6.38888889e-01 -1.66666667e-01  3.05555556e-01]\n",
    " [-5.55555556e-02  1.26893721e-16  5.55555556e-02]\n",
    " [ 5.27777778e-01  1.66666667e-01 -1.94444444e-01]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transpose转置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = np.transpose(A)\n",
    "print(G)\n",
    "\"\"\"\n",
    "[[1 4 7]\n",
    " [2 5 8]\n",
    " [3 6 9]]\n",
    "\"\"\"\n",
    "\n",
    "H = A.T\n",
    "print(H)\n",
    "\"\"\"\n",
    "[[1 4 7]\n",
    " [2 5 8]\n",
    " [3 6 9]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack堆叠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming these are two features from our dataset\n",
    "feature_1 = np.array([[123], [456], [789]])\n",
    "feature_2 = np.array([[321], [654], [987]])\n",
    "\n",
    "# Combine the two features into one matrix\n",
    "data_features = np.hstack((feature_1, feature_2))\n",
    "print(data_features)\n",
    "\"\"\"\n",
    "[[123 321]\n",
    " [456 654]\n",
    " [789 987]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize 正规化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data_features = data_features / np.linalg.norm(data_features) # L2 norm\n",
    "print(normalized_data_features)\n",
    "\"\"\"\n",
    "[[0.08022761 0.2093745 ]\n",
    " [0.2974292  0.42657609]\n",
    " [0.51463079 0.64377768]]\n",
    "\"\"\"\n",
    "\n",
    "# 确保正规化后的值在0和1之间\n",
    "normalized_data_features_minmax = (data_features - np.min(data_features)) / (np.max(data_features) - np.min(data_features))\n",
    "print(normalized_data_features_minmax)\n",
    "\"\"\"\n",
    "[[0.         0.22916667]\n",
    " [0.38541667 0.61458333]\n",
    " [0.77083333 1.        ]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weights权重点乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the weights of features\n",
    "weights = np.array([0.4, 0.6])\n",
    "\n",
    "# Calculate the weighted sum of features\n",
    "weighted_sum_features = np.dot(data_features, weights)\n",
    "print(weighted_sum_features)\n",
    "# Output: [241.8 574.8 907.8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupBy & Apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a simple dataframe\n",
    "data = {'Company': ['GOOG', 'GOOG', 'MSFT', 'MSFT', 'FB', 'FB'],\n",
    "       'Person': ['Sam', 'Charlie', 'Amy', 'Vanessa', 'Carl', 'Sarah'],\n",
    "       'Sales': [200, 120, 340, 124, 243, 350]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply groupby\n",
    "df_grouped = df.groupby('Company')\n",
    "# 要注意，groupby函数不会返回一个dataframe，返回了一个object里面包含了很多methods\n",
    "print(df_grouped.sum())\n",
    "\"\"\"\n",
    "             Person  Sales\n",
    "Company                   \n",
    "FB        CarlSarah    593\n",
    "GOOG     SamCharlie    320\n",
    "MSFT     AmyVanessa    464\n",
    "\"\"\"\n",
    "\n",
    "for key, item in df_grouped:\n",
    "    print(\"\\nGroup Key: {}\".format(key))\n",
    "    print(df_grouped.get_group(key), \"\\n\")\n",
    "\"\"\"\n",
    "Group Key: FB\n",
    "  Company Person  Sales\n",
    "4      FB   Carl    243\n",
    "5      FB  Sarah    350 \n",
    "\n",
    "\n",
    "Group Key: GOOG\n",
    "  Company   Person  Sales\n",
    "0    GOOG      Sam    200\n",
    "1    GOOG  Charlie    120 \n",
    "\n",
    "\n",
    "Group Key: MSFT\n",
    "  Company   Person  Sales\n",
    "2    MSFT      Amy    340\n",
    "3    MSFT  Vanessa    124\n",
    "\"\"\"\n",
    "\n",
    "print(df.groupby('Company').apply(lambda x: x['Sales'].max()))\n",
    "\"\"\"\n",
    "Company\n",
    "FB      350\n",
    "GOOG    200\n",
    "MSFT    340\n",
    "dtype: int64\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a dataframe\n",
    "df = pd.DataFrame({'A': ['foo', 'bar', 'foo', 'bar','foo', 'bar', 'foo', 'foo'],\n",
    "                   'B': ['one', 'one', 'two', 'three','two', 'two', 'one', 'three'],\n",
    "                   'C': np.random.randn(8),\n",
    "                   'D': np.random.randn(8)})\n",
    "\n",
    "# Define a function\n",
    "def get_sum(row):\n",
    "    return row.sum()\n",
    "\n",
    "# Apply the function \n",
    "df['sum'] = df[['C', 'D']].apply(get_sum, axis=1)\n",
    "\n",
    "print(df)\n",
    "\"\"\"\n",
    "     A      B         C         D       sum\n",
    "0  foo    one -0.343200  0.184665 -0.158535\n",
    "1  bar    one  0.058870  1.835614  1.894484\n",
    "2  foo    two  0.801743 -0.184409  0.617333\n",
    "3  bar  three  0.935406  0.124109  1.059515\n",
    "4  foo    two  0.782074  0.583470  1.365544\n",
    "5  bar    two  0.138934  0.710407  0.849341\n",
    "6  foo    one  0.364633  1.147963  1.512596\n",
    "7  foo  three -1.364677  1.719538  0.354861\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "\n",
    "# Fetch the dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "\n",
    "# create a DataFrame\n",
    "housing_df = pd.DataFrame(data=data.data, columns=data.feature_names)\n",
    "\n",
    "# Add a 'RoomPerHousehold' feature to the dataframe\n",
    "housing_df['RoomPerHousehold'] = housing_df['AveRooms'] / housing_df['AveOccup']\n",
    "\n",
    "# Perform log transformation on 'MedInc' feature (apply np.log to the column)\n",
    "df['MedInc'] = np.log(df['MedInc'])\n",
    "\n",
    "# Group by 'RoomPerHousehold' category and get the median of 'MedInc' for each group\n",
    "med_house_val = housing_df.groupby('RoomPerHousehold').apply(lambda x: x['MedInc'].median())\n",
    "\n",
    "# Print the result\n",
    "print(med_house_val)\n",
    "\n",
    "# Define income category\n",
    "housing_df['income_cat'] = pd.cut(housing_df['MedInc'],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "# Group by income category and calculate the average population\n",
    "average_population = housing_df.groupby('income_cat').apply(lambda x: x['Population'].mean())\n",
    "\n",
    "print(average_population)\n",
    "\"\"\"\n",
    "income_cat\n",
    "1    1105.806569\n",
    "2    1418.232336\n",
    "3    1448.062465\n",
    "4    1488.974718\n",
    "5    1389.890347\n",
    "dtype: float64\n",
    "\"\"\"\n",
    "\n",
    "housing_df['Age_cat'] = pd.cut(housing_df['HouseAge'],\n",
    "                               bins=[7, 14, 21, 28, 35, np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "# Group by Age_cat and calculate the average number of bedrooms\n",
    "average_value = housing_df.groupby('Age_cat').apply(lambda x: x['AveBedrms'].mean())\n",
    "\n",
    "# Plot the result as a bar chart\n",
    "average_value.plot(kind='bar', title='Average Number of Bedrooms by House Age Category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory reduce by vectorization向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Define a large array\n",
    "large_array = np.random.rand(10**6)\n",
    "\n",
    "# Python way of summing elements in an array\n",
    "start = time.time()\n",
    "print(\"Built-in list sum\", sum(large_array))  # This calculates the sum using Python's built-in function\n",
    "print(\"Time to calculate the sum in a Python list:\", time.time() - start)\n",
    "# Prints \"Time to calculate the sum in a Python list: 0.0417783260345459\"\n",
    "\n",
    "# Numpy way\n",
    "start = time.time()\n",
    "print(\"Numpy sum:\", np.sum(large_array))  # This calculates the sum using Numpy's vectorized function\n",
    "print(\"Time to calculate the sum in a Numpy list:\", time.time() - start)\n",
    "# Prints \"Time to calculate the sum in a Numpy list: 0.00037097930908203125\"\n",
    "\n",
    "# Define a large array\n",
    "large_array = np.random.rand(50000)\n",
    "\n",
    "# Compute the sum using Python's built-in function, time it, and print the result\n",
    "python_start_time = pd.Timestamp.now()\n",
    "python_sum = sum(large_array)\n",
    "python_end_time = pd.Timestamp.now()\n",
    "print(f\"Python Sum: {python_sum}, Time Taken: {python_end_time - python_start_time}\")\n",
    "\"\"\"\n",
    "Python Sum: 25041.96793358683, Time Taken: 0 days 00:00:00.020172\n",
    "\"\"\"\n",
    "\n",
    "# Compute the sum using Numpy's built-in function, time it, and print the result\n",
    "numpy_start_time = pd.Timestamp.now()\n",
    "numpy_sum = np.sum(large_array)\n",
    "numpy_end_time = pd.Timestamp.now()\n",
    "print(f\"Numpy Sum: {numpy_sum}, Time Taken: {numpy_end_time - numpy_start_time}\")\n",
    "\"\"\"\n",
    "Numpy Sum: 25041.967933586937, Time Taken: 0 days 00:00:00.000167\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### catogorical数据比int和Float节省内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Type'] = pd.Categorical(df['Type'])\n",
    "df['MedInc'] = df['MedInc'].astype('category')\n",
    "\n",
    "# 浮点Float32比float64节省内存，int32比int64节省内存，downcast用于下划数据类型\n",
    "# Downcast data type for 'AveBedrms' column\n",
    "df['AveBedrms'] = pd.to_numeric(df['AveBedrms'], downcast='float')\n",
    "df['Population'] = df['Population'].astype('int32')\n",
    "df_new = df.astype({\n",
    "    'AveBedrms': 'float32',\n",
    "    'AveRooms': 'float32',\n",
    "    'AveOccup': 'float32',\n",
    "    'Latitude': 'float32',\n",
    "    'Longitude': 'float32',\n",
    "    'MedInc': 'float32',\n",
    "    'Population': 'int32',\n",
    "    'HouseAge': 'int32'}, \n",
    "    copy=True) # True返回一个新DataFrame\n",
    "\n",
    "# Regular way\n",
    "df_copy = df[df['Population'] > 1000]\n",
    "df_copy.dropna(inplace=True)\n",
    "\n",
    "# Optimized way\n",
    "# Method Chaining将多个步骤在一行代码\n",
    "df[df['Population'] > 1000].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "# Load the California Housing dataset\n",
    "california = datasets.fetch_california_housing()\n",
    "df = pd.DataFrame(data=np.c_[california['data'], california['target']], columns=california['feature_names'] + ['target'])\n",
    "\n",
    "def memory_usage_pandas(df):\n",
    "    bytes = df.memory_usage(deep=True).sum()\n",
    "    return bytes / 1024**2  # Convert bytes to megabytes\n",
    "\n",
    "original_memory = memory_usage_pandas(df)\n",
    "\n",
    "# Optimize memory usage in Pandas using categorical data types\n",
    "# California Housing dataset does not have any Categorical features, so we will use downcasting\n",
    "df['AveBedrms'] = pd.to_numeric(df['AveBedrms'], downcast='float')\n",
    "df['AveRooms'] = pd.to_numeric(df['AveRooms'], downcast='float')\n",
    "optimized_memory = memory_usage_pandas(df)\n",
    "\n",
    "print(f'Original memory usage: {original_memory} MB')\n",
    "print(f'Optimized memory usage: {optimized_memory} MB')\n",
    "print(f'Memory saved: {original_memory - optimized_memory} MB')\n",
    "\n",
    "reduced_percent = pd.to_numeric((original_memory - optimized_memory)/original_memory *100, downcast='float')\n",
    "print(\"The memory reduced: \", reduced_percent, \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 跨学科数据例\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# DNA sequences for several genes\n",
    "data = {\n",
    "    \"Gene\": [\"Gene A\", \"Gene B\", \"Gene C\", \"Gene D\"],\n",
    "    \"Sequence\": [\"ATCGTACGA\", \"CGATCGATG\", \"TAGCTAG\", \"CGTAGCTA\"],\n",
    "    \"Discovery_Date\": pd.date_range('02/15/1935', periods=4),\n",
    "    \"Popularity_on_Social_Network\": np.array([1250, 500, 800, 2000])\n",
    "}\n",
    "\n",
    "df_genes = pd.DataFrame(data)\n",
    "print(df_genes)\n",
    "\"\"\"\n",
    "     Gene Discovery_Date  Popularity_on_Social_Network\n",
    "0  Gene A     1935-02-15                          1250\n",
    "1  Gene B     1935-02-16                           500\n",
    "2  Gene C     1935-02-17                           800\n",
    "3  Gene D     1935-02-18                          2000\n",
    "\"\"\"\n",
    "df_genes['Length'] = df_genes['Sequence'].apply(len)\n",
    "print(df_genes)\n",
    "\"\"\"\n",
    "     Gene   Sequence  Length\n",
    "0  Gene A  ATCGTACGA       9\n",
    "1  Gene B  CGATCGATG       9\n",
    "2  Gene C    TAGCTAG       7\n",
    "3  Gene D   CGTAGCTA       8\n",
    "\"\"\"\n",
    "\n",
    "data_bio = {\n",
    "    \"Gene\": [\"Gene M\", \"Gene N\", \"Gene O\", \"Gene P\"],\n",
    "    \"Sequence\": [\"TGCCGTA\", \"AATGCGT\", \"CGTACGT\", \"GGCTATG\"]\n",
    "}\n",
    "df_bio = pd.DataFrame(data_bio)\n",
    "\n",
    "# lowercase the DNA Sequence\n",
    "df_bio['Sequence'] = df_bio['Sequence'].apply(lambda x: str.lower(x))\n",
    "\n",
    "# Reverse the DNA Sequence\n",
    "df_bio['Reverse_Sequence'] = df_bio['Sequence'].apply(lambda x: x[::-1])\n",
    "\"\"\"\n",
    "     Gene Sequence Reverse_Sequence\n",
    "0  Gene M  TGCCGTA          ATGCCGT\n",
    "1  Gene N  AATGCGT          TGCGTAA\n",
    "2  Gene O  CGTACGT          TGCATGC\n",
    "3  Gene P  GGCTATG          GTATCGG\n",
    "     Gene Sequence Reverse_Sequence\n",
    "0  Gene M  TGCCGTA          ATGCCGT\n",
    "1  Gene N  AATGCGT          TGCGTAA\n",
    "2  Gene O  CGTACGT          TGCATGC\n",
    "3  Gene P  GGCTATG          GTATCGG\n",
    "\"\"\"\n",
    "# TODO: Compute the length of each reversed DNA sequence and add it to a new column, \"Reverse_Sequence_Length\"\n",
    "df_bio['Reverse_Sequence_Length'] = df_bio['Sequence'].apply(lambda x: x[::-1]).apply(len)\n",
    "\n",
    "# Suppose we need to sort the data based on its popularity on the social network\n",
    "df_gene_info_sorted = df_genes.sort_values(by='Popularity_on_Social_Network', ascending=False)\n",
    "\n",
    "# Print the DataFrame after sorting\n",
    "print(df_gene_info_sorted)\n",
    "\"\"\"\n",
    "     Gene Discovery_Date  Popularity_on_Social_Network\n",
    "3  Gene D     1935-02-18                          2000\n",
    "0  Gene A     1935-02-15                          1250\n",
    "2  Gene C     1935-02-17                           800\n",
    "1  Gene B     1935-02-16                           500\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Star dataset (Simulated data for demonstration)\n",
    "data = {\n",
    "    \"Star_ID\": np.arange(1, 5),\n",
    "    \"Right_Ascension\": [204.85, 63.70, 305.29, 45.2],\n",
    "    \"Declination\": [-29.72, 38.03, -14.78, 7.8],\n",
    "    \"Magnitude\": [2.04, 1.25, 3.17, 1.9],\n",
    "    \"Observation_Date\": pd.date_range('01/01/2020', periods=4)\n",
    "}\n",
    "\n",
    "df_stars = pd.DataFrame(data)\n",
    "print(df_stars)\n",
    "\"\"\"\n",
    "   Star_ID  Right_Ascension  Declination  Magnitude Observation_Date\n",
    "0        1           204.85       -29.72       2.04       2020-01-01\n",
    "1        2            63.70        38.03       1.25       2020-01-02\n",
    "2        3           305.29       -14.78       3.17       2020-01-03\n",
    "3        4            45.20         7.80       1.90       2020-01-04\n",
    "\"\"\"\n",
    "filter_date = pd.to_datetime('2020-01-02')\n",
    "filtered_stars = df_stars[df_stars['Observation_Date'] > filter_date]\n",
    "print(filtered_stars)\n",
    "\"\"\"\n",
    "   Star_ID  Right_Ascension  Declination  Magnitude Observation_Date\n",
    "2        3           305.29       -14.78       3.17       2020-01-03\n",
    "3        4            45.20         7.80       1.90       2020-01-04\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Social interaction data (Simulated for demonstration)\n",
    "data = {\n",
    "    \"Person\": [\"Alice\", \"Bob\", \"Charlie\", \"Dave\"],\n",
    "    \"Friends\": [10, 5, 8, 2],\n",
    "    \"Posts\": [100, 50, 80, 200]\n",
    "}\n",
    "\n",
    "df_social = pd.DataFrame(data)\n",
    "print(df_social)\n",
    "\"\"\"\n",
    "    Person  Friends  Posts\n",
    "0    Alice       10    100\n",
    "1      Bob        5     50\n",
    "2  Charlie        8     80\n",
    "3     Dave        2    200\n",
    "\"\"\"\n",
    "df_social['Posts_per_Friend'] = df_social['Posts'] / df_social['Friends']\n",
    "print(df_social)\n",
    "\"\"\"\n",
    "    Person  Friends  Posts  Posts_per_Friend\n",
    "0    Alice       10    100              10.0\n",
    "1      Bob        5     50              10.0\n",
    "2  Charlie        8     80              10.0\n",
    "3     Dave        2    200             100.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Develop and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration(Wine-quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "# Loading Dataset\n",
    "red_wine = datasets.load_dataset('codesignal/wine-quality', split='red')\n",
    "white_wine = datasets.load_dataset('codesignal/wine-quality', split='white')\n",
    "red_wine = pd.DataFrame(red_wine)\n",
    "white_wine = pd.DataFrame(white_wine)\n",
    "\n",
    "# Checking the shape of the dataset\n",
    "print(\"Red Wine Dataset Shape: \", red_wine.shape) # Red Wine Dataset Shape:  (1599, 12)\n",
    "print(\"White Wine Dataset Shape: \", white_wine.shape) # White Wine Dataset Shape:  (4898, 12)\n",
    "\n",
    "# Check Red Wine Dataset data types\n",
    "print(\"Red Wine Dataset Data Types:\")\n",
    "print(red_wine.dtypes)\n",
    "\"\"\"\n",
    "Red Wine Dataset Data Types:\n",
    "fixed acidity           float64\n",
    "volatile acidity        float64\n",
    "citric acid             float64\n",
    "residual sugar          float64\n",
    "chlorides               float64\n",
    "free sulfur dioxide     float64\n",
    "total sulfur dioxide    float64\n",
    "density                 float64\n",
    "pH                      float64\n",
    "sulphates               float64\n",
    "alcohol                 float64\n",
    "quality                 float64\n",
    "dtype: object\n",
    "\"\"\"\n",
    "\n",
    "# Check White Wine Dataset data types\n",
    "print(\"\\nWhite Wine Dataset Data Types:\")\n",
    "print(white_wine.dtypes)\n",
    "\"\"\"\n",
    "the structure is the same as in the red wine dataset\n",
    "\"\"\"\n",
    "\n",
    "# Describing Red Wine Dataset\n",
    "print(\"Red Wine Dataset Description:\")\n",
    "print(red_wine.describe())\n",
    "\"\"\"\n",
    "Red Wine Dataset Description:\n",
    "       fixed acidity  volatile acidity  ...      alcohol      quality\n",
    "count    1599.000000       1599.000000  ...  1599.000000  1599.000000\n",
    "mean        8.319637          0.527821  ...    10.422983     5.636023\n",
    "std         1.741096          0.179060  ...     1.065668     0.807569\n",
    "min         4.600000          0.120000  ...     8.400000     3.000000\n",
    "25%         7.100000          0.390000  ...     9.500000     5.000000\n",
    "50%         7.900000          0.520000  ...    10.200000     6.000000\n",
    "75%         9.200000          0.640000  ...    11.100000     6.000000\n",
    "max        15.900000          1.580000  ...    14.900000     8.000000\n",
    "\n",
    "[8 rows x 12 columns]\n",
    "\"\"\"\n",
    "\n",
    "# Unique values\n",
    "print(\"\\nUnique values in Red Wine Dataset:\")\n",
    "print(red_wine.nunique())\n",
    "\"\"\"\n",
    "Unique values in Red Wine Dataset:\n",
    "fixed acidity            96\n",
    "volatile acidity        143\n",
    "citric acid              80\n",
    "residual sugar           91\n",
    "chlorides               153\n",
    "free sulfur dioxide      60\n",
    "total sulfur dioxide    144\n",
    "density                 436\n",
    "pH                       89\n",
    "sulphates                96\n",
    "alcohol                  65\n",
    "quality                   6\n",
    "dtype: int64\n",
    "\"\"\"\n",
    "\n",
    "# Describing White Wine Dataset\n",
    "print(\"\\nWhite Wine Dataset Description:\")\n",
    "print(white_wine.describe())\n",
    "\"\"\"\n",
    "White Wine Dataset Description:\n",
    "       fixed acidity  volatile acidity  ...      alcohol      quality\n",
    "count    4898.000000       4898.000000  ...  4898.000000  4898.000000\n",
    "mean        6.854788          0.278241  ...    10.514267     5.877909\n",
    "std         0.843868          0.100795  ...     1.230621     0.885639\n",
    "min         3.800000          0.080000  ...     8.000000     3.000000\n",
    "25%         6.300000          0.210000  ...     9.500000     5.000000\n",
    "50%         6.800000          0.260000  ...    10.400000     6.000000\n",
    "75%         7.300000          0.320000  ...    11.400000     6.000000\n",
    "max        14.200000          1.100000  ...    14.200000     9.000000\n",
    "\n",
    "[8 rows x 12 columns]\n",
    "\"\"\"\n",
    "\n",
    "# Unique values\n",
    "print(\"\\nUnique values in White Wine Dataset:\")\n",
    "print(white_wine.nunique())\n",
    "\"\"\"\n",
    "Unique values in White Wine Dataset:\n",
    "fixed acidity            68\n",
    "volatile acidity        125\n",
    "citric acid              87\n",
    "residual sugar          310\n",
    "chlorides               160\n",
    "free sulfur dioxide     132\n",
    "total sulfur dioxide    251\n",
    "density                 890\n",
    "pH                      103\n",
    "sulphates                79\n",
    "alcohol                 103\n",
    "quality                   7\n",
    "dtype: int64\n",
    "\"\"\"\n",
    "# Check missing values in Red Wine Dataset\n",
    "print(\"Missing values in Red Wine Dataset:\")\n",
    "print(red_wine.isnull().sum()) # There are no null values in all columns\n",
    "\n",
    "\n",
    "# Check missing values in White Wine Dataset\n",
    "print(\"\\nMissing values in White Wine Dataset:\")\n",
    "print(white_wine.isnull().sum()) # There are no null values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot for Red Wine\n",
    "plt.hist(red_wine.quality, bins=10, color='red', alpha=0.7) # alpha=opaque不透明1，transparent透明0\n",
    "plt.xlabel('Quality')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Quality Distribution for Red Wine')\n",
    "plt.show()\n",
    "\n",
    "# Plot for White Wine\n",
    "plt.hist(white_wine.quality, bins=10, color='skyblue', alpha=0.7)\n",
    "plt.xlabel('Quality')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Quality Distribution for White Wine')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, theta, alpha, iterations):\n",
    "    \"\"\"\n",
    "    x -- input dataset\n",
    "    y -- target dataset\n",
    "    theta -- initial parameters\n",
    "    alpha -- learning rate\n",
    "    iterations -- the number of times to execute the algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    m = y.size # number of data points\n",
    "    cost_list = [] # list to store the cost function value at each iteration\n",
    "    theta_list = [theta] # list to store the values of theta at each iteration\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        # calculate our prediction based on our current theta\n",
    "        prediction = np.dot(x, theta)\n",
    "        \n",
    "        # compute the error between our prediction and the actual values\n",
    "        error = prediction - y\n",
    "        \n",
    "        # calculate the cost function\n",
    "        cost = 1 / (2*m) * np.dot(error.T, error)\n",
    "        \n",
    "        # append the cost to the cost_list\n",
    "        cost_list.append(np.squeeze(cost)) # 降维2D到1D\n",
    "        \n",
    "        # calculate the gradient descent and update the theta\n",
    "        theta = theta - (alpha * (1/m) * np.dot(x.T, error))\n",
    "        \n",
    "        # append the updated theta to the theta_list\n",
    "        theta_list.append(theta)\n",
    "    \n",
    "    # return the final values of theta, list of all theta, and list of all costs, respectively \n",
    "    return theta, theta_list, cost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt\n",
    "import datasets\n",
    "\n",
    "# Load Wine Quality Dataset\n",
    "red_wine = datasets.load_dataset('codesignal/wine-quality', split='red')\n",
    "red_wine = pd.DataFrame(red_wine)\n",
    "\n",
    "# Only consider the 'alcohol' column as a predictive feature for now\n",
    "x = pd.DataFrame(red_wine['alcohol'])\n",
    "y = red_wine['quality']\n",
    "\n",
    "# Splitting datasets into training and testing datasets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0) \n",
    "# random state是同一个值时，样本的洗牌方式是一样的，这样可以保证结果的可重复性，用于再现\n",
    "\n",
    "# We set our parameters to start at 0\n",
    "theta = np.zeros(x_train.shape[1]).reshape(-1, 1)\n",
    "\n",
    "# Define the number of iterations and alpha value\n",
    "alpha = 0.0001\n",
    "iters = 1000\n",
    "\n",
    "# Applying Gradient Descent\n",
    "y_train = np.array(y_train).reshape(-1, 1)\n",
    "g, theta_list, cost_list = gradient_descent(x_train, y_train, theta, alpha, iters)\n",
    "# g是在train数据上迭代出的最终theta\n",
    "\n",
    "print(cost_list)\n",
    "plt.plot(range(1, iters + 1), cost_list, color='blue')\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "plt.grid()\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Cost (J)')\n",
    "plt.title('Convergence of gradient descent')\n",
    "plt.show()\n",
    "\n",
    "y_test = np.array(y_test).reshape(-1, 1)\n",
    "g_test, theta_test_list, cost_test_list = gradient_descent(x_test, y_test, g, alpha, iters)\n",
    "# 带入g到test数据，观察Cost的下降\n",
    "\n",
    "plt.plot(range(1, iters + 1), cost_test_list, color='brown')\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "plt.grid()\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Cost (J)')\n",
    "plt.title('Convergence of gradient descent on the test dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "\n",
    "# Load the wine dataset\n",
    "import datasets\n",
    "red_wine = datasets.load_dataset('codesignal/wine-quality', split='red')\n",
    "red_wine = pd.DataFrame(red_wine) \n",
    "\n",
    "# Select features and target variable\n",
    "features = red_wine.drop('quality', axis=1) # 除target以外全是feature\n",
    "features = red_wine[['fixed acidity', 'volatile acidity']]\n",
    "target = red_wine['quality']\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate and fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(features_train, target_train)\n",
    "\n",
    "# Predict the test features\n",
    "predictions = model.predict(features_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = metrics.mean_squared_error(target_test, predictions)\n",
    "print('Mean Squared Error:', mse) # Mean Squared Error: 0.39002514396395416\n",
    "\n",
    "r2_score = metrics.r2_score(target_test, predictions)\n",
    "print('R-squared:', r2_score) # R-squared: 0.4031803412796231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot target vs prediction\n",
    "plt.scatter(target_test, predictions, color='blue')\n",
    "# Plot the ideal prediction line (with zero error)\n",
    "plt.plot([target_test.min(), target_test.max()], [target_test.min(), target_test.max()], 'k--', lw=2)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the wine dataset\n",
    "import datasets\n",
    "import pandas as pd\n",
    "red_wine = datasets.load_dataset('codesignal/wine-quality', split='red')\n",
    "red_wine = pd.DataFrame(red_wine)\n",
    "\n",
    "# Convert the quality ratings to binary (Good - 1 and Not Good - 0)\n",
    "red_wine['quality'] = red_wine['quality'].apply(lambda quality : 1 if quality >= 7 else 0)\n",
    "\n",
    "# Split the dataset into features and target variable\n",
    "X = red_wine.drop('quality', axis=1)\n",
    "y = red_wine['quality']\n",
    "\n",
    "# Split the dataset into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Create a Logistic Regression object\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model using the training sets\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Print the learned parameters\n",
    "print(\"Model Coefficients: \", lr.coef_[0])\n",
    "print(\"Intercept: \", lr.intercept_[0])\n",
    "\"\"\"\n",
    "[[-0.02641816 -3.24280912 -0.04024957  0.07795443 -1.26020881  0.02151089\n",
    "  -0.01866486 -1.04040183 -2.50766981  2.00156001  0.9266963 ]]\n",
    "[-1.77875604]\n",
    "\"\"\"\n",
    "# X = beta_0 + beta_1 * X_1 + ...beta_n * X_n\n",
    "# coef_ = beta_1, beta_2, ... , beta_n\n",
    "# intercept_ = beta_0\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Import metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model accuracy\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred))\n",
    "# Accuracy:  0.8875\n",
    "\n",
    "# Model Precision\n",
    "print(\"Precision: \", metrics.precision_score(y_test, y_pred))\n",
    "# Precision:  0.5172413793103449\n",
    "\n",
    "# Model Recall\n",
    "print(\"Recall: \", metrics.recall_score(y_test, y_pred))\n",
    "# Recall:  0.2727272727272727\n",
    "\n",
    "# Model F1-Score\n",
    "print(\"F1 Score: \", metrics.f1_score(y_test, y_pred))\n",
    "# F1 Score:  0.3571428571428571\n",
    "\n",
    "# Model AUC\n",
    "print(\"AUC: \", metrics.roc_auc_score(y_test, y_pred))\n",
    "# AUC:  0.6198930481283422\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE/MSE/RMSE/R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "# In our example, fitted is a numpy array that our linear regression model predicted for wine quality\n",
    "fitted = np.array([3.6, 2.7, 2.4]) \n",
    "\n",
    "# While actual is a numpy array containing the real wine qualities\n",
    "actual = np.array([3.5, 2.9, 2.6]) \n",
    "\n",
    "# For calculating MAE, pass the actual and predicted arrays to mean_absolute_error()\n",
    "mae = metrics.mean_absolute_error(actual, fitted)\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "# Mean Absolute Error (MAE): 0.16666666666666666 = (0.1+0.2+0.2)/3\n",
    "\n",
    "# For calculating MSE, use the mean_squared_error function\n",
    "mse = metrics.mean_squared_error(actual, fitted)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "# Mean Squared Error (MSE): 0.029999999999999995\n",
    "\n",
    "# RMSE is calculated as the square root of MSE, using the np.sqrt() function\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "# Root Mean Squared Error (RMSE): 0.1732050807568877\n",
    "\n",
    "# For calculating the R-squared value, use the r2_score function\n",
    "r2 = metrics.r2_score(actual, fitted)\n",
    "print(f\"R-squared: {r2}\")\n",
    "# R-squared: 0.7857142857142857"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "# Load the Red Wine Quality Data\n",
    "wine = datasets.load_dataset('codesignal/wine-quality', split='red')\n",
    "wine = pd.DataFrame(wine)\n",
    "\n",
    "# Separate Features and Target\n",
    "X = wine.drop('quality', axis=1)\n",
    "Y = wine['quality']\n",
    "\n",
    "# Split the data into training and test data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Train the Model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Make prediction\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mae = metrics.mean_absolute_error(Y_test, Y_pred)\n",
    "mse = metrics.mean_squared_error(Y_test, Y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = metrics.r2_score(Y_test, Y_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "# Mean Absolute Error (MAE): 0.4696330928661111\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "# Mean Squared Error (MSE): 0.384471197820124\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "# Root Mean Squared Error (RMSE): 0.6200574149384265\n",
    "print(f\"R-squared: {r2}\")\n",
    "# R-squared: 0.32838876395802286"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy/Precision/Recall/F1 Score/AUC-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Let y_test be a numpy array with the actual wine quality classes ('good' or 'not good') for the test dataset\n",
    "y_test = np.array(['not good', 'good', 'good', 'not good', 'good'])\n",
    "\n",
    "# And let pred be a numpy array with the predicted classes by our model for the test dataset\n",
    "pred = np.array(['not good', 'good', 'not good', 'good', 'good'])\n",
    "\n",
    "# For calculating Accuracy\n",
    "accuracy = metrics.accuracy_score(y_test, pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# For calculating Precision, use the precision_score function\n",
    "# Note: It considers 'good' as the positive class by default (this can be changed using the pos_label parameter)\n",
    "precision = metrics.precision_score(y_test, pred, pos_label=\"good\")\n",
    "print(f\"Precision: {precision}\")\n",
    "\n",
    "# For calculating Recall\n",
    "recall = metrics.recall_score(y_test, pred, pos_label=\"good\")\n",
    "print(f\"Recall: {recall}\")\n",
    "\n",
    "# For calculating F1 Score\n",
    "f1 = metrics.f1_score(y_test, pred, pos_label=\"good\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# For computing AUC-ROC, we need the probabilities of the positive class ('good'), let's assume y_proba as an array of these probabilities \n",
    "y_proba = np.array([0.1, 0.7, 0.3, 0.8, 0.7])\n",
    "auc_roc = metrics.roc_auc_score(y_test, y_proba)\n",
    "print(f\"AUC-ROC: {auc_roc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import datasets\n",
    "\n",
    "# Load the Red Wine Quality Data\n",
    "wine = datasets.load_dataset('codesignal/wine-quality', split='red')\n",
    "wine = pd.DataFrame(wine)\n",
    "\n",
    "# Transform the quality column into binary labels\n",
    "wine['quality'] = np.where(wine[\"quality\"] >= wine[\"quality\"].quantile(0.75), \n",
    "                            'good', 'not good')\n",
    "\n",
    "# Transform the labels into numerical form\n",
    "le = LabelEncoder()\n",
    "wine['quality'] = le.fit_transform(wine['quality'])\n",
    "\n",
    "# Separate Features and Target\n",
    "X = wine.drop('quality', axis=1)\n",
    "Y = wine['quality']\n",
    "\n",
    "# Split into training and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=42)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    " \n",
    "# Fit the logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "print(f\"Accuracy: {metrics.accuracy_score(Y_test, Y_pred)}\")\n",
    "print(f\"Precision: {metrics.precision_score(Y_test, Y_pred)}\")\n",
    "print(f\"Recall: {metrics.recall_score(Y_test, Y_pred)}\")\n",
    "print(f\"F1-Score: {metrics.f1_score(Y_test, Y_pred)}\")\n",
    "\n",
    "proba_pred = model.predict_proba(X_test)[:, 1]  # Probabilities of the positive class \n",
    "print(f\"AUC-ROC: {metrics.roc_auc_score(Y_test, proba_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold Cross-Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = LinearRegression()\n",
    "# clf represents an instance of a machine learning model you've already constructed (e.g., clf = LinearRegression())\n",
    "scores = cross_val_score(clf, X, y, cv=5) # k=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "# Import the dataset\n",
    "red_wine = datasets.load_dataset('codesignal/wine-quality', split='red')\n",
    "red_wine_df = pd.DataFrame(red_wine)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = red_wine_df.corr(method='pearson', min_periods=10) # method = pearson, kendall, spearman\n",
    "# min_periods说明两个feature之间计算相关性，至少需要10个数据点，说明除去missingvalue之外至少还有10个数据点\n",
    "\n",
    "# Print the correlation matrix\n",
    "print(corr)\n",
    "\"\"\"\n",
    "                      fixed acidity  volatile acidity  ...   alcohol   quality\n",
    "fixed acidity              1.000000         -0.256131  ... -0.061668  0.124052\n",
    "volatile acidity          -0.256131          1.000000  ... -0.202288 -0.390558\n",
    "citric acid                0.671703         -0.552496  ...  0.109903  0.226373\n",
    "residual sugar             0.114777          0.001918  ...  0.042075  0.013732\n",
    "chlorides                  0.093705          0.061298  ... -0.221141 -0.128907\n",
    "free sulfur dioxide       -0.153794         -0.010504  ... -0.069408 -0.050656\n",
    "total sulfur dioxide      -0.113181          0.076470  ... -0.205654 -0.185100\n",
    "density                    0.668047          0.022026  ... -0.496180 -0.174919\n",
    "pH                        -0.682978          0.234937  ...  0.205633 -0.057731\n",
    "sulphates                  0.183006         -0.260987  ...  0.093595  0.251397\n",
    "alcohol                   -0.061668         -0.202288  ...  1.000000  0.476166\n",
    "quality                    0.124052         -0.390558  ...  0.476166  1.000000\n",
    "\n",
    "[12 rows x 12 columns]\n",
    "\"\"\"\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Draw the heatmap\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm', vmin=-1, vmax=1)\n",
    "# annot添加标签，cmap更改颜色，vmin和vmax调整颜色比例\n",
    "\n",
    "plt.title('Correlation heatmap for Red Wine features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datasets\n",
    "\n",
    "# Load the dataset\n",
    "red_wine = datasets.load_dataset('codesignal/wine-quality', split='red')\n",
    "red_wine_df = pd.DataFrame(red_wine)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = red_wine_df.corr()\n",
    "\n",
    "# Print the correlation matrix with a precision of 2 decimal places\n",
    "print(corr.round(2))\n",
    "\n",
    "# Focus only on 'quality'\n",
    "corr_quality = corr_matrix[['quality']].drop('quality')\n",
    "corr_quality['quality'] = corr_quality[abs(corr_quality[\"quality\"]) > 0.3 ]\n",
    "\n",
    "sns.heatmap(corr_quality, annot=True, fmt=\".2f\")\n",
    "plt.title('Correlation heatmap for Wine Quality and most related features')\n",
    "plt.show()\n",
    "\n",
    "# Create a heatmap\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\")\n",
    "plt.title('Correlation heatmap for the Red Wine Dataset')\n",
    "plt.show()\n",
    "\n",
    "# Let's look at two features: 'pH' and 'fixed acidity'\n",
    "# Calculate 'pH' against 'fixed acidity' to find out its correlation\n",
    "feature_corr = red_wine_df['pH'].corr(red_wine_df['fixed acidity'])\n",
    "# Calculate 'density' against 'quality' to find out its correlation\n",
    "feature_corr = red_wine_df['quality'].corr(red_wine_df['density'])\n",
    "\n",
    "# Print the correlation\n",
    "print(\"\\nThe correlation of 'fixed acidity' with 'pH' is %.3f\\n\" % (feature_corr))\n",
    "\n",
    "# Display the correlation graphically via a scatter plot\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.scatterplot(x='fixed acidity', y='pH', data=red_wine_df, hue='quality')\n",
    "plt.title('Scatterplot showing correlation between \\'pH\\' and \\'Fixed Acidity\\'')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting & Underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplify overfitting and underfitting \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Generate some data\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(40, 1) ** 2\n",
    "y =  (10 - 1. / (x.ravel() + 0.1)) + np.random.randn(40)\n",
    "\n",
    "# Define a function to fit the model\n",
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree), LinearRegression(**kwargs))\n",
    "# make_pipeline封装了两个步骤，最简单例子：make_pipeline(StandardScaler(), LogisticRegression())\n",
    "\n",
    "# Fit the model\n",
    "X_test = np.linspace(-0.1, 1.1, 500)[:, None]\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(x.ravel(), y, color='black')\n",
    "axis = plt.axis()\n",
    "for degree in [1, 3, 30]:\n",
    "    y_test = PolynomialRegression(degree).fit(x, y).predict(X_test)\n",
    "    plt.plot(X_test.ravel(), y_test, label='degree={0}'.format(degree))\n",
    "plt.xlim(-0.1, 1.0)\n",
    "plt.ylim(-2, 12)\n",
    "plt.legend(loc='best');\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "import datasets\n",
    "red_wine = datasets.load_dataset('codesignal/wine-quality', split='red')\n",
    "red_wine = pd.DataFrame(red_wine)\n",
    "\n",
    "# Separate features and target\n",
    "X = red_wine.drop(columns='quality')\n",
    "y = pd.cut(red_wine['quality'], bins=[0, 6.5, 10], labels=['bad', 'good'])\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "logistic = LogisticRegression(solver='saga', tol=0.01)\n",
    "pipe = make_pipeline(logistic)\n",
    "\n",
    "# Set up the grid\n",
    "param_grid = {\n",
    "    'logisticregression__C': np.logspace(-2, 2, 5),\n",
    "    'logisticregression__penalty': ['l1', 'l2'],\n",
    "}\n",
    "\n",
    "# Initiate Grid search with cross-validation\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, verbose=0)\n",
    "# GridSearch在指定网格param_grid上穷举，返回5-fold的交叉验证分数最高 模型\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print('Best parameters: ', grid.best_params_)\n",
    "# Best parameters:  {'logisticregression__C': 0.09999999999999999, 'logisticregression__penalty': 'l2'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised 8 lessons 37 practices"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
